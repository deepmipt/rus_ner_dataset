{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Collecting emoji',\n",
       " 'deeppavlov 0.0.5 requires tensorflow==1.8.0, which is not installed.',\n",
       " 'Installing collected packages: emoji',\n",
       " 'Successfully installed emoji-0.5.0']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!!pip3 install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import csv\n",
    "import numpy as np\n",
    "from emoji import UNICODE_EMOJI\n",
    "import dateutil.parser\n",
    "import html\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "DATA_INPUT_FILE = '/data/NER/VectorX/documents_description_objects_100K_random.json'\n",
    "DATA_ALL_OUTPUT_FILE = '/data/NER/VectorX/dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "san_tokenizer = re.compile(r\"[\\w']+|[^\\w ]\")\n",
    "\n",
    "def tokenize(s):\n",
    "    return san_tokenizer.findall(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_emoji(s):\n",
    "    for emoji in UNICODE_EMOJI:\n",
    "        if emoji in s:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "_bad_beginnings = ['[id', 'Фото', 'Смотрите', 'Скачать', 'Оригинал взят']\n",
    "_bad_substrings = ['!!', '...']\n",
    "def check(t: str) -> bool:\n",
    "    for bb in _bad_beginnings:           \n",
    "        if t.startswith(bb):\n",
    "            return False\n",
    "    for bs in _bad_substrings:\n",
    "        if bs in t:\n",
    "            return False\n",
    "    \n",
    "    if has_emoji(t):\n",
    "        return False\n",
    "    \n",
    "    return t.count('(') == t.count(')')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 0 articles\n",
      "CPU times: user 601 µs, sys: 302 µs, total: 903 µs\n",
      "Wall time: 576 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lines = []\n",
    "data = []\n",
    "\n",
    "san_dquoter = re.compile(r\"(«|»|“|”|``|'|„|“)\")\n",
    "\n",
    "NumberInt = re.compile(r'NumberInt\\((\\d+)\\)')\n",
    "ISODate = re.compile(r'ISODate\\((\"[^\"]+\")\\)')\n",
    "\n",
    "quoter = re.compile(r'& ?[qQ]uot ?;')\n",
    "amper = re.compile(r'& ?amp ?; ?')\n",
    "words_re = re.compile(r'\\w+')\n",
    "\n",
    "photo_re = re.compile(r'[Фф]ото: [\\.a-zA-Z0-9\\-]+\\.[a-zA-Z]+')\n",
    "\n",
    "counter = 0\n",
    "\n",
    "descr_set = set()\n",
    "\n",
    "with open(DATA_INPUT_FILE) as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip()        \n",
    "        m1 = NumberInt.findall(line)\n",
    "        if m1:\n",
    "            line = NumberInt.sub(m1[0], line)\n",
    "        m2 = ISODate.findall(line)\n",
    "        if m2:\n",
    "            line = ISODate.sub(m2[0], line)\n",
    "            \n",
    "        lines.append(line)\n",
    "        if line == '}':\n",
    "#             print('\\n'.join(lines))\n",
    "            j_orig = json.loads('\\n'.join(lines))\n",
    "            pub_date = j_orig['rss_pubDate']\n",
    "        \n",
    "            assert len(j_orig) == 2\n",
    "            j = j_orig['rss']            \n",
    "            assert len(j) == 1\n",
    "            j = j['channel']\n",
    "            assert len(j) == 1\n",
    "            j = j['item']\n",
    "            \n",
    "            raw_description = j['description']\n",
    "            guid = j['guid']\n",
    "            objects = j['objects'].get('object', [])\n",
    "\n",
    "            \n",
    "            if raw_description and isinstance(raw_description, str):\n",
    "                # assert isinstance(description, str), f'\"{description}\" is not str in object {j_orig}'\n",
    "                description = html.unescape(quoter.sub('&quot;', amper.sub('&', photo_re.sub('', raw_description))))\n",
    "                description = san_dquoter.sub('\"', description)\n",
    "                \n",
    "                words_only = tuple(words_re.findall(description.lower()))\n",
    "                if words_only not in descr_set:\n",
    "                    descr_set.add(words_only)\n",
    "                    tokenized = tokenize(description)\n",
    "                    description_tokenized = ' '.join(tokenized)\n",
    "                    data.append([guid, description, description_tokenized, len(tokenized), pub_date, objects])            \n",
    "                    counter += 1\n",
    "\n",
    "            lines = []\n",
    "print(f'Read {counter} articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'guid': 82387388,\n",
       " 'description': 'Проблемное снабжению холодной и горячей воды в некоторых домах Винницы проявляется через дырявые трубы. Заявляют в & quot; Винницагортеплоэнерго & quot;. Подсчитывают, только за полгода одном из домов в городе, вытекло около 2000 кубометров воды. Предприятие разработало жесткий алгоритм устранения проблемы.\\n& Quot; Провели aнaлиз, и оказалось, что около 20 000 кубических метров было потерь во внутренних тепловых мережaх при домивкaх. Это потому, что ЖЭКи проводили плохо експлуaтaцию. Приклaд - Вaтутинa, 35. Затем мы знaйшлы еще подобные инциденты по другим улицам. Трaсы тaм в жaхливому Стaнь. Нa одном метре трубы по 5-6 резиновых стяжек. Трубы те уже експлуaтувaты неможнa, a они используются. Втрaты воды идут. Только по одному дому около 2000 кубометров вытекло зa полгода. Это понaд 140000 гривен мы потеряли по одному помещению. A подобных домов в еще бaгaто & quot ;, - розповидaе Вiнниця.info Юлиaн Мигaенко, нaчaльник службы безопасности кaдрового зaбезпечення & quot; Винницагортеплоэнерго & quot ;.\\n& Quot; Винницагортеплоэнерго & quot; зaключило договоренность с городской влaдою о том, если обнаруживают aвaрийни ситуaции, будут сообщать нa & quot; круглосуточную вaрту & quot;.\\n& Quot; Будем звонить диспетчерaм. В течение двух часов мешкaнци домов мaють зaпaстися водой. Затем дом от & amp; rsquo; соединяем от водопостaчaння. И пока ЖЭК не устранит проблему, мы дом не пидключaемо & quot;, - подчеркнул Нигаенко.\\nВiнниця.info',\n",
       " 'objects': {'object': [{'type': 'money', 'content': '140.000 UAH'},\n",
       "   {'type': 'person', 'content': 'Юлиан Мигаенко'}]}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.1 s, sys: 167 ms, total: 19.3 s\n",
      "Wall time: 19.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "cv = CountVectorizer()\n",
    "texts = [description_tokenized.lower() for guid, description, description_tokenized, token_count, pub_date, objects in data]\n",
    "counts = cv.fit_transform(texts)    \n",
    "\n",
    "single_doc_words_count = csr_matrix(single_doc_words).multiply(csr_matrix(counts > 0)).sum(axis=1) \n",
    "uniq_words_counts = (counts > 0).sum(axis=1)\n",
    "single_doc_words_ratio = np.array(single_doc_words_count / np.maximum(1, uniq_words_counts))[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.55 s, sys: 620 ms, total: 8.17 s\n",
      "Wall time: 10.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "with open(DATA_ALL_OUTPUT_FILE, 'w') as fw:\n",
    "    cw = csv.writer(fw)\n",
    "    cw.writerow(['guid', 'descriptions', 'description_tokenized', 'token_count', 'pub_date', 'objects', 'single_doc_words_ratio'])\n",
    "    for row, sdwr in zip(data, single_doc_words_ratio):\n",
    "        cw.writerow(row + [sdwr])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to file /data/NER/VectorX/toloka_2017-08-01.tsv\n",
      "wrote 3762 articles\n",
      "CPU times: user 6.88 s, sys: 113 ms, total: 6.99 s\n",
      "Wall time: 7.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "target_date = datetime(2017, 8, 1).replace(tzinfo=pytz.UTC).date()\n",
    "# target_date = datetime(2017, 8, 31).replace(tzinfo=pytz.UTC).date()\n",
    "toloka_file = DATA_TOLOKA_FILE_TEMPLATE.format(target_date)\n",
    "\n",
    "print(f'Writing to file {toloka_file}')\n",
    "\n",
    "c = 0\n",
    "with open(toloka_file, 'w') as fw:\n",
    "    cw = csv.writer(fw, delimiter='\\t')\n",
    "    cw.writerow(['INPUT:guid', 'INPUT:orig', 'INPUT:input'])\n",
    "    for guid, description, description_tokenized, token_count, pub_date, objects, sdwr in data:        \n",
    "        row_date = dateutil.parser.parse(pub_date).date()\n",
    "        if 40 < int(token_count) < 1750 and row_date == target_date and float(sdwr) < 0.15:\n",
    "            c += 1\n",
    "            cw.writerow([guid, description, description_tokenized])\n",
    "print(f'wrote {c} articles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tf18",
   "language": "python",
   "name": ".venv_tf18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
