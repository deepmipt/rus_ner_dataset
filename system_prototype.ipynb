{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_train/82207990.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_train/82157720.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_train/82229530.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_train/82218580.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_train/82178650.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_train/82217490.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_test/84272795.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_test/84296221.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_test/84261598.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_test/84261780.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_test/84303229.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_test/84312867.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_test/84326311.txt\n",
      "Exception \"Entity is in two sentences\" for file /data/NER/VectorX/NE5_test/84270836.txt\n"
     ]
    }
   ],
   "source": [
    "import rus_ner\n",
    "\n",
    "rus_ner.slurp_NE5_annotated_data('/data/NER/VectorX/NE5_train', 'train')\n",
    "rus_ner.slurp_NE5_annotated_data('/data/NER/VectorX/NE5_test', 'test')\n",
    "train = rus_ner.get_supervised_data('train')\n",
    "test = rus_ner.get_supervised_data('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud = rus_ner.get_unsupervised_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "from pathlib import Path\n",
    "import csv\n",
    "from typing import List, Tuple, Callable, Iterable\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "import pandas as pd\n",
    "\n",
    "NE5_dataset = Path('/data/NER/VectorX/NE5_train')\n",
    "# NE5_dataset = Path('/data/NER/Academic Datasets/NE5/Collection5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spans(text, sentences):\n",
    "    sents_span = []\n",
    "\n",
    "    last_index = -1\n",
    "    for s in sentences:\n",
    "        i = text[last_index+1:].index(s)\n",
    "        span_start = last_index + 1 + i\n",
    "        span_end = last_index + 1 + i + len(s)\n",
    "\n",
    "        sents_span.append((s, span_start, span_end))\n",
    "        last_index = span_end\n",
    "\n",
    "    return sents_span\n",
    "\n",
    "\n",
    "# def get_spans(text, tokenizer, subs_map=None):\n",
    "#     if subs_map is None:\n",
    "#         subs_map = {}\n",
    "        \n",
    "#     wi = 0\n",
    "#     words = tokenizer(text)\n",
    "#     word_origs = [subs_map.get(w, w) for w in words]\n",
    "#     words_span = []\n",
    "\n",
    "#     i = 0\n",
    "#     while i < len(text):\n",
    "#         if text[i:].startswith(word_origs[wi]):\n",
    "#             words_span.append((words[wi], i, i+len(word_origs[wi])))\n",
    "#             i += len(word_origs[wi])\n",
    "#             wi += 1\n",
    "#             continue\n",
    "#         i += 1\n",
    "\n",
    "#     return words_span\n",
    "\n",
    "\n",
    "def split_sentences_preserve_tags(text : str, \n",
    "                                  tags_with_spans: Iterable[Tuple[str, int, int, str]], \n",
    "                                  sentence_tokenizer: Callable[[str], Iterable[str]]):\n",
    "    tags_with_spans = sorted(tags_with_spans, key=lambda x: x[1])  # excess\n",
    "    sents = sentence_tokenizer(text)\n",
    "#     print(text)\n",
    "#     sents_span = get_spans(text, sentence_tokenizer)\n",
    "    sents_span = get_spans(text, sents)\n",
    "\n",
    "    per_sentence_tags = []\n",
    "    for orig_s, s_start, s_end in sents_span:\n",
    "        sent_tags = []\n",
    "        s = orig_s.strip()\n",
    "        offset = orig_s.index(s)\n",
    "        s_start = s_start - offset\n",
    "        s_end = s_end - offset\n",
    "        \n",
    "        for tag, t_start, t_end, value in tags_with_spans:\n",
    "            if s_start <= t_start <= s_end:\n",
    "                assert s_start <= t_end <= s_end, 'Entity is in two sentences'\n",
    "                assert s[t_start - s_start: t_end - s_start] == value, 'Tag value does not equal to sent spans'\n",
    "                sent_tags.append((tag, t_start - s_start, t_end - s_start, value))\n",
    "        per_sentence_tags.append((s, sent_tags))\n",
    "    return per_sentence_tags\n",
    "\n",
    "\n",
    "def spans_to_bio(word_spans: List[Tuple[str, int, int]], tags: List[Tuple[int, int, str]]):\n",
    "    '''supposed that tags do not overlap'''\n",
    "    tags = sorted(tags, key=lambda x: x[0])\n",
    "    for t1, t2 in zip(tags[:-1], tags[1:]):\n",
    "        assert t1[1] <= t2[0], 'Tags must not overlap'\n",
    "    \n",
    "    bio = []\n",
    "    for t_start, t_end, tag in tags:\n",
    "        last_tag = 'O'    \n",
    "        for w, w_start, w_end in word_spans:\n",
    "            if t_start <= w_start < t_end or t_start < w_end < t_end:\n",
    "                prefix = 'I-' if last_tag == tag else 'B-'\n",
    "                bio.append((w, prefix + tag))\n",
    "                last_tag = tag\n",
    "            else:\n",
    "                bio.append((w, 'O'))\n",
    "                last_tag = 'O'\n",
    "    return bio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(':memory:')\n",
    "\n",
    "c = conn.cursor()\n",
    "\n",
    "c.execute('''CREATE TABLE sentences (article_id INT, \n",
    "                                     value TEXT)''')\n",
    "c.execute('''CREATE TABLE tags (sentence_id INT, \n",
    "                                start_index INT, \n",
    "                                end_index INT, \n",
    "                                tag TEXT)''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_texts = []\n",
    "all_annotations = []\n",
    "for ann_fn in NE5_dataset.glob('*.ann'):\n",
    "    c = conn.cursor()\n",
    "#     ann_fn = Path('/data/NER/VectorX/NE5_train/82180290.ann')\n",
    "    article_id = ann_fn.name[:-len('.ann')]\n",
    "    text_fn = Path(str(ann_fn)[:-len('.ann')] + '.txt')\n",
    "    \n",
    "    text = text_fn.read_text()\n",
    "    all_texts.append({'article_id': article_id, 'text': text})\n",
    "    \n",
    "    annotations = []\n",
    "    with ann_fn.open() as f:\n",
    "        cr = csv.reader(f, delimiter='\\t', quotechar='|')\n",
    "        for row in cr:\n",
    "            assert len(row) == 3, row\n",
    "            _, tag_info, tag_value = row\n",
    "            tag, start_index, end_index = tag_info.split()\n",
    "            annotations.append((tag, int(start_index), int(end_index), tag_value))\n",
    "\n",
    "    st = split_sentences_preserve_tags(text, annotations, sent_tokenize)\n",
    "    \n",
    "    \n",
    "    for s, stags in st:\n",
    "        r = c.execute('''INSERT INTO sentences(article_id, value) VALUES(?,?)''', (article_id, s))\n",
    "        s_id = r.lastrowid\n",
    "        for st, ss, se, sv in stags:\n",
    "            c.execute('INSERT INTO tags(sentence_id, start_index, end_index, tag) VALUES (?, ?, ?, ?)', (s_id, ss, se, st))\n",
    "\n",
    "    conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = c.execute('SELECT sentences.oid, sentences.article_id, sentences.value, tags.start_index, tags.end_index, tags.tag FROM sentences LEFT OUTER JOIN tags ON sentences.oid = tags.sentence_id')\n",
    "d = r.fetchall()\n",
    "\n",
    "data = pd.DataFrame(d, columns='sentences.oid sentences.article_id sentences.value tags.start_index tags.end_index tags.tag'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = []\n",
    "for sid, d in data.groupby('sentences.oid'):\n",
    "    s = d['sentences.value'].iloc[0]\n",
    "    tags = []\n",
    "    for _, row in d.iterrows():\n",
    "        if not pd.isna(row['tags.start_index']):\n",
    "            tags.append((int(row['tags.start_index']), int(row['tags.end_index']), row['tags.tag']))\n",
    "    sents.append((s, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text, tags = sents[1]\n",
    "nltk_subs_map = {\"''\": '\"', '``': '\"'}\n",
    "\n",
    "word_spans = get_spans(text, word_tokenize, nltk_subs_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_spans"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_tf18",
   "language": "python",
   "name": ".venv_tf18"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
